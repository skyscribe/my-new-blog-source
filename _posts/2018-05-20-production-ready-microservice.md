---
layout: post
title: 标准化和生产环境就绪的微服务
comments: true
categories: [design, architecture]
tags: [design, architecture, microservice, standardization]
---

微服务作为一种分布式的软件架构风格几乎已经席卷软件开发的各个角落；尽管它从被明确提出之后也很快经历了大量的质疑、批判乃至否定；
然而背后实践这一符合康威定律的简单（将问题分而治之）而又复杂（需要解决服务本身引起的的许多复杂的基础设施问题）的架构风格的组织却与日俱增。
其原因也不过是随着云计算技术的逐步深入，分布式架构成为最简单的选择，因为大部分情况下处于成本的考量，我们更需要水平扩展而不是垂直扩展；
微服务引入过程中的**一些痛点也随着越来越多云原生应用的开源组件的出现而得到缓解**。尝试将微服务实践标准化以降低实施成本的努力也变得不再是“空中楼阁”了，
走的人多了，必然有些更容易走的"直路"显现出来。

<!--more-->

## 为什么需要标准化

Susan J.Fowler作为一名来自Uber的站点可靠性工程师(Site Reliability Engineer)见证了Uber自身将其庞大而又复杂的单体API剥离成逾千个微服务的，
并达到超过85%的微服务团队并无配备SRE这一角色也能确保自己的服务运行良好。同时她也见证了微服务团队（尤其是大型微服务架构）中SRE工程师所面临的巨大挑战：

- 对SRE本身的能力要求是全方位的，他/她必须是包括软件工程、系统工程、软件架构等多个方面的综合性专家
- 在有成百上千个微服务组成的企业应用中，大部分的团队更本不可能拥有SRE的资源；SRE也不太可能熟悉哪怕系统中大部分微服务的关键细节
- SRE需要对系统整体的可靠性负责，然而任何一个微服务的SLA水平下降都可能带来整个系统的SLA下降；在大多数团队不配备SRE的情况下，
如何保证日常的开发中新引入的功能不会破坏整体的可靠性、可用性和性能的关键指标，或者如何做有效的回归和可靠性测试，并在发现可能的失败情况下，
及早地回退引起整体质量下降的服务代码，都是一个看起来几乎不可能的任务

Susan希望他们可以找到一种简单而又直接的方法来定义微服务实践的规范程度，即一种简单而又可靠的标准，使得组织中的每一个微服务团队都可以仅仅需要遵循这些标准就可以交付符合SLA要求的服务，
同时还保有微服务架构本身带来的诸如自由选择编程语言、技术栈的优势。这一套标准其实是用一系列检查列表（CheckList)的方式提供，微服务团队在提交自己的实现之前检查这些列表，
确保所有的实现都能满足列表的要求的情况下才将代码引入到生产环境中；这样整个系统的SLA就可以得到保证。

## 基本原则

Susan他们给出的标准化微服务的标准大概可以划分到八个大的基本原则之下，每个微服务必须满足
- 稳定
- 可靠
- 可扩展
- 容错
- 性能良好
- 被合理监控
- 有良好的文档
- 容灾就绪

在这些大的原则之下，Susan他们还详细定义了每一个原则里面的详细列表，并要求每一个原则必须是**可以量化**以方便提供度量结果，
从而可以极大地提高微服务系统的可用性。只有**满足了这里列出的所有的条件**的微服务，才可以被认为是生产就绪的。

### 微服务生态系统

微服务从来都不是孤立从在的，它们被构建、运行和交互的环境就是它们生存的环境；大型的微服务系统的生存环境就像是一大片森林，一个沙漠或者一个大海；
将相关的**整个环境考虑为一个生态系统**往往更有利于我们采用微服务风格来架构我们的应用系统。

设计良好的可持续的微服务生态系统往往会尽力用一个基础设施层将底层的细节和上层的微服务应用相隔离开；这样微服务的开发人员可以像开发一个单体应用一样，
使用他们喜欢的技术和编程语言，自己决定服务内部的软件架构；并不需要过多考虑底层基础设施这些细节，比如操作系统、网络、存储等复杂的基础知识。
自然地，将这些基础设施保持地稳定、高效、可扩展并能良好地容错就成为一个非常基本的需求。

微服务的生态系统可以简单地划分为四个层次，尽管有时候层次之间的界限并不是那么清晰而绝对
- 硬件层
- 通信层
- 软件应用平台
- 微服务层

最底层的部分是硬件层，它负责处理诸如实际的物理资源，包括真实硬件、网络节点，乃至数据中心的机架、交换机或者SDN网络、存储设备等；这些设施可以是自己搭建的数据中心，
也可以是从其它的云服务商处租赁来，比如AWS EC2、Google GCP、MS Azure等。所有这些相关的细节被硬件层所管理。
硬件之上的操作系统，以及相关的配置，资源隔离和抽象设置，主机层面的日志、监控也需要被这一层统一提供；
具体的系统选择和日志、监控工具可能由企业应用具体的环境来选择决定。
某种意义上可以将这一硬件层看作是云时代的网络操作系统。

硬件层之上的是通信层；该层本质上参与了上面两层的交互，因为**所有微服务乃至软件应用平台的交互**都被这一层所处理。
从严格分层的角度来看，似乎这一层有些定义不太良好。幸运的是，基本的要素是清晰的。
它需要处理诸如网络设置、DNS解析、RPC设施、API端点控制、服务发现和注册、负载均衡等微服务底层治理设施。

作为第三层的软件平台层负责处理所有微服务共享的内部工具、共享的基础服务等。
这一层必须提供整个系统范围内共享的基础应用软件设施，使得微服务团队**不需要自己设计和维护他们自身微服务逻辑之外的复杂基础软件**；
从而微服务团队可以专注于自身需要实现的业务逻辑，而不是比较底层的软件技术。
设计良好的软件平台需要提供内部开发者共同使用的方便的**内部开发工具,自动化构建和测试平台**，中心化的自动部署和发布方案，比如一些DevOps工具，自动发布工具，
微服务层面的日志、监控工具等。

微服务层处于这个生态系统的最上层，主要负责其职责范围内的业务逻辑和API实现，并和底层的硬件、服务治理设施、通用应用软件平台隔离开。
唯一一个没有和下面几层完全隔离的就是微服务自身的配置。一种常见的做法是将所有的微服务的配置都用中心化的方式来管理和控制；然而这在大型的微服务软件中可能带来巨大的问题，
因为当微服务人员需要修改其配置的时候，往往不得不修改其下面几层的文件（因为被中心化托管），由于微服务数量庞大，开发人员往往不知应该改动哪里或者遗漏重要的配置。
这种情况下，更合理的方案是将各个微服务的配置放在自己的代码库中，然后让底层的工具或者平台可以访问这些配置文件。

## 可靠性和稳定性
微服务架构其实给软件系统的可靠性和稳定性带来的更多的挑战，这些挑战本质上是分布式软件架构内生的矛盾引起的，而我们又没有办法回到单体架构的时代。
从整个生态系统的角度来考虑，任何一个环节、要素的可靠性损失都会传导到相关的上下游，从而使得整个系统的可用性下降。

### 构建稳定而又可靠的微服务的基本原则
微服务架构给开发人员引入了巨大的灵活性，并使得快速的功能迭代成为可能。各个微服务团队可以选用他们自己所熟悉的编程语言、库和微观架构，
只要提供定义良好的API接口即可。每个服务的设计和功能特性都可能在随时发生着巨大的变化，因而整个系统中的任何一个微服务因为设计不良或者考虑不周引起的稳定性下降或者可靠性降低，
都会对整个系统的可用性和可靠性带来毁灭性的影响。因此我们天然期望**微服务的任何一次代码的合入都不会引起**服务的可靠性、可用性水平的下降。

构建和维护一个可以被称之为稳定而又可靠的微服务，意味着任何一次功能的添加、问题的修复或者代码的重构及演进不会带来整个系统的可靠性和稳定性的降低。
当然变得更好是会更受欢迎的。为了发布和维护可靠的微服务，我们需要保证有良好的开发流程来支持，并且最好有一个标准化的开发周期模型使得

1. 新功能的开发或者其它原因引入的代码改动**必须经过预先定义的多个阶段的流水线检查**，确保问题尽可能在早期阶段被发现并及时回滚
2. 服务之间的**依赖失效问题可以被尽早发现并阻止**；一般认为对服务API打版本标签是一种反模式而不值得提倡，因为这样很容易引起沟通不足而导致下游微服务意外失效
3. 适当的路由检查、断路器控制应该被继承到通信层上，实现对微服务出错情况下的处理，防止引起系统性的崩溃
4. 某个特定的微服务提供的API可以被废弃或者微服务本身失效的情况下，其它的微服务不应该一并失效

稳定而又可靠的微服务开发往往经历如下的开发模型
1. 一旦开发人员提交代码的改动到仓库中的新分支（git模型），新的自动化构建任务即被创建起来执行，并做好自动化的代码检查和服务内部的测试
2. 同时改动的代码需要被仔细的审查和评审，确保可能的问题被及早发现
3. 只有当前两步的检查都没有问题的时候，新的代码才会被提交给外部的构建系统中和其它的服务做集成，打包，测试
4. 测试通过的包会被同时发送给部署的流水线做持续部署测试和提交到服务对应的仓库中

### 部署流水线

大型系统中的大部分错误通常都被认为是来源于错误的部署。在大型的开发组织中，可能有上百个微服务在按照各自的节奏做设计、开发和维护，
微服务团队之间通常很少有足够好的协调和沟通（因为康威定律决定了跨大型组织的沟通成本是极其高昂）。这种情况下，
任何一个微服务部署了错误的有问题的版本都有可能造成整个系统的停摆。这种情况下，引入复杂而又分阶段的部署流水线则成为保证微服务系统可靠、稳定的必需手段。

当一个新的微服务版本被发送到部署流水线的时候（紧接着上一节的过程）

1. 该版本暂时被标记为候选的发布版本，先放置到 Staging 阶段，并运行一系列的测试
2. 如果上述测试都通过并觉得没有太大问题，则顺序进入下一个“金丝雀部署”阶段；这里的环境其实是从正在线上运行的微服务生产环境中，
跳出一部分工作负载，让新的候选微服务版本提供服务，并观察是否有异常情况发生
3. 如果没有问题，则可以慢慢地按照既定的策略用新的微服务版本逐步替换就的服务版本

任何一个阶段发现问题，则应当必须尽快回滚，基础设施和构建服务必须能快速回溯到出问题的改动，待对应的微服务团队诊断问题进一步修复。
通常情况下，Staging 的测试环境往往和后边两个阶段的环境有比较大的区别。金丝雀发布阶段可以根据服务实例的个数或者API的流量选择重定向一小部分到新的候选服务版本；
譬如5%到10%的生产环境的情况可以被路由到新的服务实例上做处理；一旦发生错误不致于给用户带来很大的困扰，并且的出错的情况下，应该及时终止这一部署回滚到稳定的生产环境版本，
并抓取对应的日志、诊断信息，以便SRE工程师或者具体的微服务团队解决问题，重新发布。

#### Staging
Staging环境的测试虽然往往和生产环境不同，然而也需要尽量保证硬件设置上和生产环境比较接近，只是没有真正的用户流量。
由于资源的限制，在云平台上生产环境可能有数百个微服务的实例来均衡处理实际的负载，而staging环境上实例的数量可能就少得多。
用户请求的流量可以用mock的方法来模拟测试，或者用工具甚至手工测试的方法来验证。它和开发环境的本质不同是，发布到该环境的服务是可以被发布的候选版本。
开发人员**必须用同等的重视程度和优先级来应对staging环境上可能出现的错误**并及时修复。

有两种不同的方法来选择和构建Staging环境，一种是Full Staging，这种情况下，staging环境基本上就是生产环境的比例缩小的一个拷贝。
所有的微服务生态系统都被完整的运行，它可以和生产环境共享同样的基础设施、平台等，只是前后端的流量不是来自于真实的用户，
它也会有自己单独的数据库，并且永远不会和生产环境的微服务发生通信和交互。另外一种配置方式被称之为Partial Staging，
这种环境下，Staging和生产环境并没有完全做镜像隔离，即staging环境的微服务（往往是候选版本）可能会和生产环境中的微服务有进行交互，
从而使用生产环境中的下游微服务的API，当然处于数据保护的需要，生产环境中的数据不应该被Partial  Staging的微服务所修改，而只允许读取和访问操作。
由于Partial Staging环境中的候选版本的微服务可能访问生产环境中的服务，测试和配置的时候则需要更加小心一些，以防对生产环境造成比较大的破坏。

#### 金丝雀部署

金丝雀部署的想法其实是借鉴了以前矿工下矿井的时候的准备不周，由于不确定矿井中的瓦斯含量和氧气含量是否会由于人体无法感知的异常而引起死亡等事故，
矿工们往往会带一只金丝雀下井，金丝雀对环境的敏感程度远远超过人类，如果一氧化碳含量过高，金丝雀就会停止鸣叫从而矿工们便知道井下不安全。
金丝雀部署的策略和旧时矿工们所采用的策略类似，先选择一小部分真实的环境做实验，如果没有异常出现说明可以继续放心部署下去。

金丝雀部署是**将候选的微服务版本直接放入了生产环境中**使用(尽管只服务一小部分流量)，因此应该认为它是生产环境部署的一部分。
所有的监控、诊断、日志信息应该和生产环境中的配置完全相同，以便开发人员可以很方便地调试和解决可能出现的问题。如果有异常的情况出现，
自动回滚金丝雀环境也是至关重要的；因为任何的错误都意味着声场环境出了问题。

另外一个相关的问题是怎么确定一个服务的新候选版本应该在金丝雀部署环境中呆多长时间才可以被放心地放入大规模部署中。
这个其实取决于具体微服务的流量模式和具体的业务逻辑模型，我们需要根据服务的流量模式和业务模型确定一个金丝雀周期时间，
只有活过了事先定义好的周期时间后，对应的候选版本才能进一步部署到生产环境中。

#### 生产环境的部署

任何一次的构建和发布到部署环境的改动都应该尽力保证做到稳定而可靠。经过前述各个阶段的测试和检查，
最终部署到生产环境的微服务候选版本应该是被充分测试和验证过的，**任何情况下（除非是一些极端危急的情况），都不应该跳过前述的步骤**而直接放到生产环境中。
最后的部署可以是采用一次性全部部署到所有的实例上的方式，也可以采用更审慎的方式，按照某种预先指定的百分比策略逐步铺开，
比如先部署25%的实例，再慢慢到50%，75%乃至最后完全部署成功。其它的策略也可以按照服务的国家、地区、数据中心、或者混合上述这些策略来部署开来。

确保上述的开发和部署流水线被正确执行是保证微服务系统稳定、可靠发布和运行的关键，因为只有如此，
才能有把握将可能出现的问题在发布之前被尽可能早地揪出来修复掉，避免破坏生产环境的稳定、可靠运行。
对某些开发者来说，严格遵循上述的周期和流水线可能显得笨拙而缓慢，然而很多情况下，期望一天可以发布数次微服务的变更而又不破坏系统的稳定性和可靠性，
在复杂的大型微服务系统中变得几乎不可能。每隔几个小时就发生变化的微服务很少是稳定而又可靠的。

### 依赖管理
微服务架构的一大设想就是各个微服务团队可以独立地发布和演进他们的微服务；这在理论上没什么问题，现实环境中依赖却无处不在。
从设计上来说，我们可以努力减少服务之间的依赖和耦合，却永远不能一劳永逸地消灭依赖。几乎每一个微服务都有上下游的依赖，
它需要从上游的微服务接收请求，处理返回，并在处理的过程中调用下游的微服务提供的API。
**理解微服务的上下游依赖，事前仔细的规划和应对可能的依赖失效、错误**是确保服务可靠、稳定运行的关键要素之一。

**TBD**
