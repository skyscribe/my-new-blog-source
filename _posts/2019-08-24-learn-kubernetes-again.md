---
layout: post
title: 重学Kubernetes - 学张磊的《深度剖析Kubernetes》
comments: true
categories: [study, notes, tools, architecture]
tags: [kubernetes,notes,engineering,tools,container]
---

云计算从十年前的风起云涌却叫好不叫座的局面慢慢地变成了无人刻意提起的境地，这一切的幕后游戏规则改变者某种程度上来说
都是因为Kubernetes这个伟大的开源项目以及依托于它的云原生运动
([参考前文]({{"/" | relative_url }}{% post_url 2018/2018-04-26-cloud-native %})))
的蓬勃发展而激发。

<!--more-->
## 购买专栏的缘起

极客时间也在刚刚开始大规模推广不久就推出了自身Kubernetes贡献者张磊的《深度剖析Kubernetes》课程；作为一个资深付费用户，
我并没有在第一时间购买该课程，一部分原因是**出于谨慎**(显然之前是过分谨慎了才有遗珠之恨)，一部分原因则是出于人性的懒惰：
总觉得这样的内容应该网上一找一大把，官方文档又这么丰富，为什么还要专门购买？

然而一次偶然听到的InfoQ大会上张磊做的关于Kubernetes本质和演进的演讲勾起了我内心的兴趣；我不禁想**能够将这个庞大的开源项目做条分缕析、深入浅出地介绍的明明白白的技术人**开的专栏必然不简单。
于是才又“精明”地收藏了这个专栏并在节日活动的时候果断购入，然后依照自己的节奏慢慢消化吸收。

原作者的内容分为八个部分来阐述，这里也就着作者本身的顺序简单地记录下自学过程中的一些重要要素和思考。

## Kuberenetes的背景和容器技术的基本发端
第一个部分是关于历史背景的讲述，这方面的故事在好多地方都流传甚广了。简单来说Kubernetes这个项目其实是**Google内部自己已经做了很多年的分布式系统开发经验**并经历了多次迭代之后的结晶，开源给整个业界使用的。

然而这里面有个很**偶然的因素便是Docker这个容器打包项目的流行和壮大**，因为从Google本身而言，它并没有动力将自己强大的云计算基础设施开放出来给其它的云平台竞争对手使用。
徐飞在他的技术于商业案例解析的专栏中甚至提到，Google一开始可能是藏着掖着故意不开放自己的容器管理技术，
而仅仅开放了GFS、BigTable这些大数据技术，导致开源社区在基于虚拟机的方案上走了很多弯路。

商业公司上的开源与不开源的抉择其实不太容易为外人所知；但是回顾过去十数年的云计算发展历史，**Kubernetes无疑是站在了正确的赛道**上。

###　容器打包技术为什么成为颠覆者
早期的云计算技术其实都是围绕着虚拟机技术而展开的，即便是操作系统层面的虚拟化技术在十几年前就已经出现了。
Docker容器技术的出现初看起来没有太大技术本质上的创新，不过是用分层的文件系统打包应用程序，然后交给一个容器运行时后台去调度和执行。

它的流行其实是因为软件工程上的便利：传统的方式下软件工程师需要面对的是一系列复杂的软件运行时相关环境，包括采用什么样的操作系统，程序怎么样启动，如何和系统服务／系统调用打交道，如何将软件的价值交付给用户。

容器技术则提倡**开发者应该专注于业务逻辑本身**，只需要将自己的应用程序写好就行了，其它怎么运行的事情统统交给容器引擎来完成就行。
这样的思想不光成就了程序员，也成就了一系列服务于云技术底层的技术公司。

### Docker容器技术和其它既有技术的区别
Docker技术虽然脱胎于旧有的容器技术，它本身却和老的基于LXC的容器技术有着显著的区别
- LXC技术的基本侧重点是提供系统级容器，容器隔离的**沙箱内提供给开发者的是一个看起来像一个完整的操作系统的运行环境**，可谓麻雀虽小五脏俱全，用户可以像使用一个Linux服务器一样使用这个容器，可以启动多个应用服务，实现服务监控等
- Docker技术则主要提供应用程序容器，并且提倡**一个容器一个应用程序**的概念。某种程度上看一个Docker容器就是一个进程；
进程的上下文环境需要保持尽可能的简单，仅仅提供程序运行本身必要的运行时库和可执行文件等；多余的东西可以一概不要。

当然讨论到Kubernetes和云原生的语境的时候，我们已经很少去刻意关照古老的LXC容器了；
大部分情况下我们**说到容器的时候，基本就是在说应用程序容器**。

### 容器的本质是对进程的隔离和限制
容器技术本身提供的机制是基于内核提供的名字空间namespace和控制组cgroups的概念，它本身的实现也不是完美无缺的；
尤其是我们关心安全的时候，就会发现就所有同一宿主上运行的**容器必须共享内核这一限制就会带来潜在的风险**；而新型的容器技术可能会弥补这一缺陷。

由于是**基于系统调用层面的虚拟隔离**，容器技术有着相比虚拟机高很多的性能。

### 容器镜像格式的本质
具体就Docker容器所使用的aufs文件系统来说，其镜像其实是通过**分层的方式来组织的**，任何改变容器的方法必须要通过修改文件系统的挂载方式来实现。
简单的来说，容器的aufs文件系统通过层层叠加的方式描述软件和它自身的依赖，可以看作是三层组成
- 只读层存放一些不希望被程序修改的内容，比如一个busybox环境或者ubuntu文件系统的可执行文件
- init对应于一些配置文件，类似/etc/等，没有修改的时候为空
- 用户指定的可读写层，用于用户添加自己的内容，但是如果用户删除了某些文件，容器镜像只会将其设置为隐藏而不会真正的删除对应的文件

对于需要依赖于网络环境来交换镜像的应用场景而言，容器应用的**开发者需要力保容器的镜像要能够足够的小**，并且可以增量编译，因为容器格式本身也是用分层的思路来设计的。
这里一个很重要的点是需要选择一个比较小的基础镜像；而由于aufs的删除隐藏策略，删除一个臃肿的基础镜像里面不用的文件并不会使你的容器文件变小：他们只是变得不可访问了而已。

### Kubernetes承担的角色
容器化的应用开发、测试、部署方式其实对底层的管理和调度平台提出了不少要求，因为一个基于云原生理念开发的微服务系统不可避免需要有很多个容器实例来组成，各种**方方面面的需求最后被用编排这两个词**来概括，大概包含了下面这些功能
- 镜像管理和分发
- 容器实例的启动、管理和状态监控
- 多个服务实例之间的服务发现、负载均衡、扩展、发布和升级
- 其它高级功能如日志、监控、熔断、限流等

基于Google自身从Borg到Omega再到开发Kubernetes系统的长期的基础设施虚拟化管理经验，Kubernetes项目本身采用了完全基于声明式API的做法来构建一个极度强调灵活性的基础平台
- 其主要系统都是基于Object来建模的，并且对各种Object提供了丰富的REST API
- 内部各种Object之间使松散耦合的
- 请求改变状态的指令使通过**提交一个Object的目标状态**给Kubernetes平台来实现的
- Kubernetes会监控用户提交的请求，**循环调用相应的对象控制器来尝试改变对象的状态**，然后比较是否满足要求，如果未满足则会继续调度执行

### Kubernetes使一个运维工具吗？
从其API设计上来看，毋宁说它是一个**面向开发者的工具**而不是一个面向运维人员的平台。
由于其内部各个部分都是面向REST API的，因此使用者需要提供一个对象的目标状态的对象声明文件；
而既有的一些命令行工具都不过是对这一过程的封装。

因而从运维人员的角度来看，Kubernetes的命令行工具非常难用，因为它本身**不是为完成一个一个具体的管理任务的网络服务运维人员量身打造**的。



## 集群搭建和部署
这一部分相对比较简单，尽管作者也花了很大力气来介绍`kubeadm`这个强大的工具。
其实对于简单学习而言最简单的部署方式是再自己的笔记本或者服务机上安装一个`minikube`的环境，模拟一个单节点的最小化的运行环境；
如果需要搭建一个具有多个节点的集群，大抵需要遵循如下的步骤
- 准备节点机器，确认内核版本，网络可达性
- 部署集群的master节点，它上面一般不建议部署应用程序容器，而仅仅用于集群自身的管理和维护
- 部署网络插件，确保各个节点之间可以互相连通
- 部署多个集群的worker节点，这里是真正地跑应用程序容器的地方
- 部署容器存储插件，以便有状态的服务可以访问持久化存储设施

## 容器管理和编排
容器的管理和控制是Kubernetes最为核心的功能，也是初步了解Kubernetes最容易卡壳的地方，因为它提出了自身特有的一些抽象的概念。

### POD
POD是一个不太容易让人理解的概念，它**纯粹是Kubernetes抽象出来的一个概念**以方便Kubernetes的调度控制器实现复杂的逻辑。
好在我们可以用一个很简单的操作系统中的概念来类比：
- 如果将kubernetes自身看作是操作系统（它的确也是云环境下的操作系统），那么典型的Linux操作系统是由很多个进程组成的
- Docker容器所运行的程序本质上是一个可运行的进程，上面已经探讨过
- POD可以看作是逻辑上相关的一个进程组，这些**进程之间相互由比较亲密的关系**，适合于放在一起做调度；而进程组和进程组之间则关系没有那么紧密
当然这里的类比只是为了便于理解而并不是特别准确。

#### POD资源管理
从资源管理的角度来看，POD里面的所有的容器都会
- 共享同一个网络空间，即它们之间的网络默认就是互通的，甚至可以使用Unix　Domain　Socket这样依赖于本地网络的机制。
- 共享同样的磁盘存储，一个容器写入在某个路径的文件，共享同一个POD的其它容器也可以访问
- 享有同样的生存周期，即Kubernetes平台会保证当需要杀掉一个容器的时候，共享同一个POD的其它容器也会被一并杀掉

#### 特殊的Infra容器
为了实现上面的资源共享，Kubernetes平台其实提供了一个非常特殊的Infra容器并用它来串起来各个容器。
这个特殊的容器本身是用汇编语言写成的，本身只有100~200KB几乎不占用什么额外的资源。
进而一个POD**生存周期控制和它的进出网络流量都是通过这个基础设施容器**来实现的。

#### Projected Volume投射数据卷
Kubernetes里和POD紧密相关的一个重要的概念是Project Volume，它本身是**为容器提供预先定义好的数据**，这样当容器启动的时候，这些数据就可以直接使用了。

这样的投射数据卷有如下一些类型
- ConfigMap常常用于保存一些文本的配置数据，里面**不能包含二进制**数据，一般用来保存非加密数据
- Secret用于防止加密数据，其数据**本身是放在Kubernetes自身的etcd数据库**中的，一般放入用户就不能再获取到明文
- Downward API可以将POD自身的一些信息暴露给容器内部的应用程序使用，比如宿主机名字，IP，POD的名字，IP地址，标签和标记信息等；这些信息必须是容器启动前就可以从POD里面拿到的，不能是动态信息
- ServiceAccountToken则通过授权的方式允许容器里面的程序访问Kubernetes本身的API Server来获取平台自身的信息；而默认情况下POD里面的容器是无权访问底层平台的API信息的

#### POD健康检查
POD自身的运行状态是通过健康检查机制来实现的，**用户需要提供POD健康检查对应的探针**（可以是一个REST API也可以是一个命令行输出）,
而Kubernetes平台会定期检查这个探针的返回状态来决定POD是否出于正常运行状态。
如果没有定义，则Kubernetes则会根据docker容器的运行命令的结果来判断POD是否出于健康状态。

如果POD的健康检查返回的是异常状态，默认Kubernetes会尝试再同一个节点上重启该POD；同时这个恢复机制的策略可以通过修改POD的SPEC声明来指定。

由于POD本身的**自我复机制只能限定在启动的那个节点**上，万一该节点发生故障，Kubernetes也不能将其移动到另一个节点上；要实现类似的功能，需要采用更为高级的Deployment控制器来完成。

### 控制器模式
控制器模式是Kubernetes平台最基本的编排模式，其思路等同于如下的伪代码

```golang
for {
    actual_state := fetch_actual_state_of_X()
    expected_state := fetch_expected_state_of_X()
    if actual_state == expected_state {
        do_nothing()
    } else {
        execute_orchestration()
    }
}
```

不同的控制器有不同的实现，但是它们都遵循如上的处理逻辑。实际的状态来自于当前集群上，对应的API对象的实际状态，而**期望的状态则来自于用户提交的API声明**。
任何时候用户想改变对象的状态，都**不是直接提交命令完成，而是修改期望的对象状态**，然后默默等待后台的控制器异步调度对应的预定义动作，完成对象更新。

至于控制器对象本身的YAML声明可以理解为包含两个部分
- 控制器自身的定义，包括期望的状态
- 被控制对象的期望状态

#### Deployment控制器
Deployment用于控制POD的容器镜像、标签、网络设置等信息；每次更新这些内容，Deployment的控制器就会被触发确保POD被调度到期望的状态。

### 水平扩展和多副本
可扩展性是云计算给应用程序开发者提供的最大的一个灵活性之一；我们可以按照不同的负载和扩展性要求组织不同的服务到不同的POD中，进而实现按照需要自动伸缩、按需付费的理想;
这一能力也是最早的云计算想法被提出的时候，一个合格的云平台就应该提供的能力。

本来扩展性支持水平和垂直两个方向的扩展要求，水平扩展支持**增加新的服务实例**来提高服务能力，而垂直扩展则是通过添加既有服务实例的资源的方式来提升容量。
随着云平台应用的逐步深入，人们发现**水平扩展比垂直扩展更为方便和易用**，因此Kubernetes本身提供了对水平扩展的灵活支持；支持的方法是**通过ReplicaSet来完成**的。

ReplicaSet和Deployment、POD之间有着**层层递进的控制**关系，是三个关系紧密的控制器
- ReplicaSet通过其spec中定义的`replicas=x`声明，来控制`POD`对象，保证实例数目总是等于`x`
- Deployment对象则通过控制ReplicaSet实现具体的水平缩放、滚动更新等动作

所谓的`scale`命令，其实是对ReplicaSet控制的一种封装，它虽然具**有命令是控制的外壳，其内部却依然是通过上述的通用的控制器模式**来实现的。

#### 滚动更新
对于需要滚动更新的常见，Deployment其实会创建多个版本的`ReplicaSet`,并且根据期望的新旧版本的实例个数，
动态的创建、停止新旧版本的软件，实现服务的动态滚动部署。

### StatefulSet有状态服务
**Deployment控制器模式所能控制的POD默认是需要保持无状态**的，即不同的副本关闭和重启对服务提供的业务逻辑而言是没有关联的；这也是云原生模式要求的。
然而现实实际往往没有这么美好，实际情况是往往我们还有很多有状态的服务，即不同的实例之间是不能无缝替换的。
对于这种情况，Kubernetes提供了`StatefulSet`这一对象来描述和管理。

其设计将状态分为两种
- 拓扑状态，即某些服务实例需要按照特定的顺序来启动，它们的拓扑结构上产生的依赖和状态
- 存储状态，即不同的服务实例需要不同的存储数据，比如数据库的主从关系等

#### 拓扑状态
基于拓扑顺序而言的有状态应用而言，我们可以利用headless service的概念，通过拿到service名字只会，直接用label选择规则就可以定位到对应的service POD实例的IP，而不需要经过virtual IP机制。

这一策略能行得通是因为POD在被创建的时候，**每个POD都被赋予一个带编号的名字**(比如web-1/web-2/等)，并且这些编号从1开始依次增加，严格按照顺序创建。
销毁的时候，也需要按照顺利进行。

通过这种方式，Kubernetes可以为有状态的服务提供一个独一无二的DNS名字，这样服务使用方就**可以根据名字来解析到对应的POD IP进行访问**，即使是POD的IP地址可以在运行的时候被改变也不至于造成太大的影响。

#### 存储状态
对于需要访问有状态存储的服务而言，Kubernetes通过如下的服务对象来支持
- Persistent Volume Claim用于指定持久化卷访问声明，这是一个关于访问需求的描述对象
- Persistent Volume持久化卷用于提供具体的持久化访问能力

PV和PVC的概念设计非常类似于应用程序开发中的**接口和实现分离的具体实现；PV是具体的实现，而PVC则充当接口**。
应用程序的POD声明需要绑定一个具体的PVC对象，Kubernetes平台就会自动完成POD需要的持久化卷和实际可用的持久化卷之间的匹配。

在完成对应的PVC绑定之后，**PVC的名字就会和StatefulSet的名字绑定在一起**完成统一编号，形成类似于`<pvcname>-<satefulset>-1`这样的名字。

当POD被删除的时候，**PVC对应的存储卷里面的内容并不会丢失**而是被持久化保存起来，这样下次POD再次被Statefulset创建起来的时候，它还可以访问到之前对应的那个PVC里面的内容。
它的工作原理如下
1. 当POD被删除的时候，StatefulSet会留意到对应的POD停止了
2. 控制器会重新尝试创建对应编号的POD，并且在创建之后，尝试重新关联相同编号的PVC

### DaemonSet守护进程
**TBD**
